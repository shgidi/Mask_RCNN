{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to put models in ./logs/{task}/{model}\n",
    "# tasks = [banana_flowers, avocado_flowers]\n",
    "# data folders: data (images) should be at ~/data/Rahan/{task}\n",
    "\n",
    "task = 'banana_flowers'\n",
    "\n",
    "# update the following command from via to coco if you want to load coco format data\n",
    "data_types ={'banana_flowers':'via','avocado_flowers':'via','cucumber_flowers_3':'coco'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mrcnn.config import Config\n",
    "\n",
    "class FlowersConfig(Config):\n",
    "    \"\"\"Configuration for training on the toy shapes dataset.\n",
    "    Derives from the base Config class and overrides values specific\n",
    "    to the toy shapes dataset.\n",
    "    \"\"\"\n",
    "        \n",
    "    # Give the configuration a recognizable name\n",
    "    NAME = task\n",
    "    \n",
    "    # Train on 1 GPU and 8 images per GPU. We can put multiple images on each\n",
    "    # GPU because the images are small. Batch size is 8 (GPUs * images/GPU).\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 4\n",
    "\n",
    "    # Number of classes (including background)\n",
    "    NUM_CLASSES = 1 + 1  # background + 1\n",
    "\n",
    "    # Use small images for faster training. Set the limits of the small side\n",
    "    # the large side, and that determines the image shape.\n",
    "    IMAGE_MIN_DIM = 256\n",
    "    IMAGE_MAX_DIM = 512 # larger images for small flowers like avocado\n",
    "\n",
    "    # Use smaller anchors because our image and objects are small\n",
    "    RPN_ANCHOR_SCALES = (8, 16, 32, 64, 128)  # anchor side in pixels\n",
    "\n",
    "    # Reduce training ROIs per image because the images are small and have\n",
    "    # few objects. Aim to allow ROI sampling to pick 33% positive ROIs.\n",
    "    TRAIN_ROIS_PER_IMAGE = 32\n",
    "\n",
    "    # Use a small epoch since the data is simple\n",
    "    STEPS_PER_EPOCH = 100\n",
    "\n",
    "    # use small validation steps since the epoch is small\n",
    "    VALIDATION_STEPS = 5\n",
    "\n",
    "class InferenceConfig(FlowersConfig):\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "    \n",
    "config = FlowersConfig()\n",
    "inference_config = InferenceConfig()\n",
    "# config.display()\n",
    "\n",
    "# augmentations\n",
    "seq = iaa.Sequential([\n",
    "    iaa.Fliplr(0.5), # horizontal flips\n",
    "    iaa.Crop(percent=(0, 0.1)), # random crops\n",
    "    # Small gaussian blur with random sigma between 0 and 0.5.\n",
    "    # But we only blur about 50% of all images.\n",
    "    iaa.Sometimes(0.5,iaa.GaussianBlur(sigma=(0, 0.5))),\n",
    "    # Strengthen or weaken the contrast in each image.\n",
    "    iaa.ContrastNormalization((0.75, 1.5)),\n",
    "    # Add gaussian noise.\n",
    "    # For 50% of all images, we sample the noise once per pixel.\n",
    "    # For the other 50% of all images, we sample the noise per pixel AND\n",
    "    # channel. This can change the color (not only brightness) of the pixels.\n",
    "    iaa.AdditiveGaussianNoise(loc=0, scale=(0.0, 0.05*255), per_channel=0.5),\n",
    "    # Make some images brighter and some darker.\n",
    "    # In 20% of all cases, we sample the multiplier once per channel,\n",
    "    # which can end up changing the color of the images.\n",
    "    iaa.Multiply((0.8, 1.2), per_channel=0.2),\n",
    "    # Apply affine transformations to each image.\n",
    "    # Scale/zoom them, translate/move them, rotate them and shear them.\n",
    "    iaa.Affine(\n",
    "        scale={\"x\": (0.8, 1.2), \"y\": (0.8, 1.2)},\n",
    "        translate_percent={\"x\": (-0.2, 0.2), \"y\": (-0.2, 0.2)},\n",
    "        rotate=(-25, 25),\n",
    "        shear=(-8, 8)\n",
    "    )\n",
    "], random_order=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading banana_flowers data\n",
      "Images: 139\n",
      "Classes: ['BG', 'flower']\n",
      "Images: 44\n",
      "Classes: ['BG', 'flower']\n",
      "inference mode\n",
      "load model from /home/gidish/cloned_libs/Mask_RCNN/logs/banana_flowers20180714T1529/mask_rcnn_banana_flowers_0025.h5\n",
      "Re-starting from epoch 25\n"
     ]
    }
   ],
   "source": [
    "# loading the model and data\n",
    "\n",
    "f=Flower_detection(task) #avocado_flower\n",
    "f.load_data(data_types[f.task])\n",
    "f.model = f.load_model('last', 'inference') #load last model from designated folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_image, _, _, _, _ = modellib.load_image_gt(f.test_dataset, inference_config, \n",
    "                           image_id=0, use_mini_mask=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1 images\n",
      "image                    shape: (512, 512, 3)         min:    0.00000  max:  255.00000  uint8\n",
      "molded_images            shape: (1, 512, 512, 3)      min: -123.70000  max:  151.10000  float64\n",
      "image_metas              shape: (1, 14)               min:    0.00000  max:  512.00000  int64\n",
      "anchors                  shape: (1, 65472, 4)         min:   -0.17712  max:    1.05188  float32\n"
     ]
    }
   ],
   "source": [
    "results = f.model.detect([original_image], verbose=1)\n",
    "# Returns a list of dicts, one dict per image. The dict contains:\n",
    "#         rois: [N, (y1, x1, y2, x2)] detection bounding boxes\n",
    "#         class_ids: [N] int class IDs\n",
    "#         scores: [N] float probability scores for the class IDs\n",
    "#         masks: [H, W, N] instance binary masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
